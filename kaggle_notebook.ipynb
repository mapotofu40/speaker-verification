{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba995531",
   "metadata": {},
   "source": [
    "# Speaker Verification with ECAPA-TDNN on Kaggle GPU\n",
    "\n",
    "This notebook implements speaker verification using the ECAPA-TDNN architecture on Kaggle's GPU.\n",
    "\n",
    "## Table of Contents\n",
    "1. Setup and Installation\n",
    "2. Repository and Environment Configuration\n",
    "3. Dataset and Model Configuration\n",
    "4. Model Training\n",
    "5. Optional: MUSAN Augmentation Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "29a7752a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T19:47:06.217992Z",
     "iopub.status.busy": "2025-06-01T19:47:06.217665Z",
     "iopub.status.idle": "2025-06-01T19:47:12.504887Z",
     "shell.execute_reply": "2025-06-01T19:47:12.504168Z",
     "shell.execute_reply.started": "2025-06-01T19:47:06.217966Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (6.0.2)\n",
      "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (0.13.1)\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.9)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.26.4)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.2.2)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.25.1)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa) (25.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->soundfile) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->soundfile) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->soundfile) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->soundfile) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->soundfile) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->soundfile) (2.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->soundfile) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->soundfile) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->soundfile) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->soundfile) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->soundfile) (2024.2.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\n",
      "Package installation complete!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchaudio PyYAML soundfile librosa wandb\n",
    "!pip install matplotlib numpy tqdm\n",
    "\n",
    "print(\"Package installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7f54167d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T19:47:12.506740Z",
     "iopub.status.busy": "2025-06-01T19:47:12.506517Z",
     "iopub.status.idle": "2025-06-01T19:47:13.385945Z",
     "shell.execute_reply": "2025-06-01T19:47:13.385047Z",
     "shell.execute_reply.started": "2025-06-01T19:47:12.506720Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n",
      "Cloning into 'speaker-verification'...\n",
      "remote: Enumerating objects: 56, done.\u001b[K\n",
      "remote: Counting objects: 100% (56/56), done.\u001b[K\n",
      "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
      "remote: Total 56 (delta 21), reused 45 (delta 10), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (56/56), 35.29 KiB | 5.04 MiB/s, done.\n",
      "Resolving deltas: 100% (21/21), done.\n",
      "\n",
      "Directory structure:\n",
      ".:\n",
      "cli.py\t ecapa_tdnn\t   kaggle_notebook.ipynb  requirements.txt  train.py\n",
      "config\t evaluate.py\t   models\t\t  tester\t    utils\n",
      "dataset  global_config.py  README.md\t\t  trainer\t    verify.py\n",
      "\n",
      "./config:\n",
      "defaults.py  __init__.py\n",
      "\n",
      "./models:\n",
      "ecapa_tdnn.py  feature_extractor.py  __init__.py  modules.py\n",
      "\n",
      "./utils:\n",
      "augment.py  config.py  data.py\t__init__.py  metrics.py  training.py\n",
      "\n",
      "Project directory: /kaggle/working/speaker-verification\n",
      "Current working directory: /kaggle/working/speaker-verification\n",
      "\n",
      "Python path:\n",
      "  /kaggle/working/speaker-verification\n",
      "  /kaggle/working\n",
      "  /kaggle/lib/kagglegym\n",
      "  /kaggle/lib\n",
      "  /usr/lib/python311.zip\n",
      "  /usr/lib/python3.11\n",
      "  /usr/lib/python3.11/lib-dynload\n",
      "  \n",
      "  /usr/local/lib/python3.11/dist-packages\n",
      "  /usr/lib/python3/dist-packages\n",
      "  /usr/local/lib/python3.11/dist-packages/IPython/extensions\n",
      "  /usr/local/lib/python3.11/dist-packages/setuptools/_vendor\n",
      "  /root/.ipython\n",
      "  /tmp/tmpro502e0_\n"
     ]
    }
   ],
   "source": [
    "# Setup working directory and clone repository\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Start fresh in working directory\n",
    "%cd /kaggle/working\n",
    "!rm -rf speaker-verification\n",
    "\n",
    "# Clone repository\n",
    "!git clone https://github.com/mapotofu40/speaker-verification.git\n",
    "\n",
    "# Move config.py to avoid name conflict\n",
    "!mv speaker-verification/config.py speaker-verification/global_config.py\n",
    "\n",
    "# Set up Python path properly\n",
    "project_dir = Path('/kaggle/working/speaker-verification').absolute()\n",
    "\n",
    "# Clean up sys.path\n",
    "sys.path = [p for p in sys.path if 'speaker-verification/speaker-verification' not in p]\n",
    "\n",
    "# Add project directory to Python path if not already present\n",
    "if str(project_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(project_dir))\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir(project_dir)\n",
    "\n",
    "print(\"\\nDirectory structure:\")\n",
    "!ls -R\n",
    "\n",
    "print(f\"\\nProject directory: {project_dir}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"\\nPython path:\")\n",
    "for p in sys.path:\n",
    "    print(f\"  {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4ce5bba5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T19:47:13.387415Z",
     "iopub.status.busy": "2025-06-01T19:47:13.387149Z",
     "iopub.status.idle": "2025-06-01T19:47:13.399342Z",
     "shell.execute_reply": "2025-06-01T19:47:13.398826Z",
     "shell.execute_reply.started": "2025-06-01T19:47:13.387391Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported successfully!\n",
      "\n",
      "GPU available: True\n",
      "GPU device: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Clear any remaining path duplicates\n",
    "sys.path = list(dict.fromkeys(sys.path))\n",
    "\n",
    "# Verify imports work\n",
    "try:\n",
    "    from config.defaults import BASE_CONFIG\n",
    "    from models.ecapa_tdnn import SpeakerVerificationModel\n",
    "    from models.feature_extractor import FeatureExtractor\n",
    "    from utils.data import VietnamCelebDataset, ValidationPairDataset, collate_fn\n",
    "    from utils.training import train_model\n",
    "    from utils.metrics import compute_eer, cosine_similarity\n",
    "    from tqdm import tqdm\n",
    "    print(\"All modules imported successfully!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"Import error: {e}\")\n",
    "    print(\"\\nCurrent directory structure:\")\n",
    "    !pwd\n",
    "    !ls -R\n",
    "    print(f\"\\nPython path:\")\n",
    "    for p in sys.path:\n",
    "        print(f\"  {p}\")\n",
    "    raise\n",
    "\n",
    "# Verify GPU availability\n",
    "print(f\"\\nGPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba27bd8",
   "metadata": {},
   "source": [
    "## Weights & Biases Setup\n",
    "\n",
    "Before running the training, you need to:\n",
    "1. Create a free account at https://wandb.ai if you haven't already\n",
    "2. Get your API key from https://wandb.ai/authorize\n",
    "3. When you run the cell below, you'll see a prompt that says \"wandb: Please enter your credentials to login to wandb\"\n",
    "4. Paste your API key and press Enter\n",
    "\n",
    "Your API key will be securely stored for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a2f25bc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T19:47:13.401324Z",
     "iopub.status.busy": "2025-06-01T19:47:13.401129Z",
     "iopub.status.idle": "2025-06-01T19:47:20.049699Z",
     "shell.execute_reply": "2025-06-01T19:47:20.048941Z",
     "shell.execute_reply.started": "2025-06-01T19:47:13.401309Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vibrant-firebrand-13</strong> at: <a href='https://wandb.ai/mapotofu-hanoi-university-of-science-and-technology/speaker-verification/runs/0xixvidl' target=\"_blank\">https://wandb.ai/mapotofu-hanoi-university-of-science-and-technology/speaker-verification/runs/0xixvidl</a><br> View project at: <a href='https://wandb.ai/mapotofu-hanoi-university-of-science-and-technology/speaker-verification' target=\"_blank\">https://wandb.ai/mapotofu-hanoi-university-of-science-and-technology/speaker-verification</a><br>Synced 2 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250601_194628-0xixvidl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/speaker-verification/wandb/run-20250601_194713-qggcy8qu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mapotofu-hanoi-university-of-science-and-technology/speaker-verification/runs/qggcy8qu' target=\"_blank\">brisk-waterfall-14</a></strong> to <a href='https://wandb.ai/mapotofu-hanoi-university-of-science-and-technology/speaker-verification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mapotofu-hanoi-university-of-science-and-technology/speaker-verification' target=\"_blank\">https://wandb.ai/mapotofu-hanoi-university-of-science-and-technology/speaker-verification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mapotofu-hanoi-university-of-science-and-technology/speaker-verification/runs/qggcy8qu' target=\"_blank\">https://wandb.ai/mapotofu-hanoi-university-of-science-and-technology/speaker-verification/runs/qggcy8qu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/mapotofu-hanoi-university-of-science-and-technology/speaker-verification/runs/qggcy8qu?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7acaa374d690>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure paths for Kaggle\n",
    "config = BASE_CONFIG.copy()\n",
    "config['paths'].update({\n",
    "    'checkpoint_dir': '/kaggle/working/checkpoints',\n",
    "    'log_dir': '/kaggle/working/logs',\n",
    "    'cache_dir': '/kaggle/working/cache'\n",
    "})\n",
    "\n",
    "# Create necessary directories\n",
    "for path in config['paths'].values():\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Login to Weights & Biases\n",
    "import wandb\n",
    "\n",
    "# This will prompt you to enter your API key\n",
    "wandb.login(key = \"890ec82b435e34992eb8da6f0b05ae313d701245\")\n",
    "\n",
    "# Initialize wandb run\n",
    "wandb.init(\n",
    "    project=\"speaker-verification\",\n",
    "    config={\n",
    "        \"architecture\": \"ECAPA-TDNN\",\n",
    "        \"dataset\": \"VietnamCeleb\",\n",
    "        **config['training'],  # Add training config\n",
    "        **config['audio'],     # Add audio processing config\n",
    "        **config['model']      # Add model architecture config\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8808d731",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T19:49:28.410758Z",
     "iopub.status.busy": "2025-06-01T19:49:28.410465Z",
     "iopub.status.idle": "2025-06-01T19:49:28.421337Z",
     "shell.execute_reply": "2025-06-01T19:49:28.420742Z",
     "shell.execute_reply.started": "2025-06-01T19:49:28.410738Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# MUSAN dataset configuration\n",
    "musan_config = {\n",
    "    'data_root': '/kaggle/input/musan-dataset/musan',  # Update this path to your MUSAN dataset location\n",
    "    'noise_dir': 'noise',\n",
    "    'music_dir': 'music',\n",
    "    'speech_dir': 'speech',\n",
    "    'sample_rate': config['audio']['sample_rate'],\n",
    "    'min_snr_db': 5,\n",
    "    'max_snr_db': 20\n",
    "}\n",
    "\n",
    "# Update augmentation configuration with MUSAN parameters\n",
    "augment_config = {\n",
    "    'enabled': True,\n",
    "    'speed_perturb': True,\n",
    "    'musan_path': musan_config['data_root'],\n",
    "    'noise_prob': 0.6,  # Probability of applying noise augmentation\n",
    "    'noise_types': {\n",
    "        'noise': 0.3,  # background noise probability (maps to noise_dir)\n",
    "        'music': 0.4,  # music interference probability (maps to music_dir)\n",
    "        'speech': 0.3  # speech/babble noise probability (maps to speech_dir)\n",
    "    },\n",
    "    'noise_snr_range': [musan_config['min_snr_db'], musan_config['max_snr_db']],  # Use correct parameter name\n",
    "    'reverb_prob': 0.5,\n",
    "    'musan_noise_prob': 0.6  # Probability of using MUSAN noise vs. Gaussian noise\n",
    "}\n",
    "\n",
    "# Validate MUSAN paths\n",
    "musan_path_map = {\n",
    "    'noise': os.path.join(musan_config['data_root'], musan_config['noise_dir']),\n",
    "    'music': os.path.join(musan_config['data_root'], musan_config['music_dir']),\n",
    "    'speech': os.path.join(musan_config['data_root'], musan_config['speech_dir'])\n",
    "}\n",
    "\n",
    "for noise_type, path in musan_path_map.items():\n",
    "    if not os.path.exists(path):\n",
    "        logger.warning(f\"MUSAN {noise_type} path does not exist: {path}\")\n",
    "    else:\n",
    "        logger.info(f\"MUSAN {noise_type} path validated: {path}\")\n",
    "\n",
    "# Update wandb config with MUSAN parameters\n",
    "wandb.config.update({\n",
    "    'musan': {\n",
    "        'noise_prob': augment_config['noise_prob'],\n",
    "        'noise_types': list(augment_config['noise_types'].keys()),\n",
    "        'noise_snr_range': augment_config['noise_snr_range']\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f86ecb45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T19:49:31.961407Z",
     "iopub.status.busy": "2025-06-01T19:49:31.961132Z",
     "iopub.status.idle": "2025-06-01T19:49:33.076752Z",
     "shell.execute_reply": "2025-06-01T19:49:33.076043Z",
     "shell.execute_reply.started": "2025-06-01T19:49:31.961388Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "data_config = {\n",
    "    'data_root': '/kaggle/input/vietnam-celeb-dataset/full-dataset',  # Update with your dataset path\n",
    "    'metadata_file': '/kaggle/input/vietnam-celeb-dataset/full-dataset/speaker-metadata.tsv',\n",
    "    'train_utterance_file': '/kaggle/input/asv-output/cleaned_utterances.txt',\n",
    "    'val_easy_utterance_file': '/kaggle/input/vietnam-celeb-dataset/vietnam-celeb-e.txt',\n",
    "    'val_hard_utterance_file': '/kaggle/input/vietnam-celeb-dataset/vietnam-celeb-h.txt'\n",
    "}\n",
    "\n",
    "# Create feature extractor\n",
    "feature_extractor = FeatureExtractor(\n",
    "    sample_rate=config['audio']['sample_rate'],\n",
    "    n_mels=config['audio']['n_mels']\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = VietnamCelebDataset(\n",
    "    data_root=data_config['data_root'],\n",
    "    metadata_file=data_config['metadata_file'],\n",
    "    utterance_file=data_config['train_utterance_file'],\n",
    "    feature_extractor=feature_extractor,\n",
    "    cache_dir=config['paths']['cache_dir'],\n",
    "    use_cache=config['cache']['enabled'],\n",
    "    augment=augment_config['enabled'],\n",
    "    augment_config=augment_config,  # Now includes MUSAN configuration\n",
    "    musan_path=augment_config['musan_path']  # Add MUSAN path\n",
    ")\n",
    "\n",
    "# Create validation datasets with the new ValidationPairDataset\n",
    "val_easy_dataset = ValidationPairDataset(\n",
    "    data_root=data_config['data_root'],\n",
    "    metadata_file=data_config['metadata_file'],\n",
    "    pair_file=data_config['val_easy_utterance_file'],\n",
    "    feature_extractor=feature_extractor,\n",
    "    cache_dir=config['paths']['cache_dir'],\n",
    "    use_cache=config['cache']['enabled']\n",
    ")\n",
    "\n",
    "val_hard_dataset = ValidationPairDataset(\n",
    "    data_root=data_config['data_root'],\n",
    "    metadata_file=data_config['metadata_file'],\n",
    "    pair_file=data_config['val_hard_utterance_file'],\n",
    "    feature_extractor=feature_extractor,\n",
    "    cache_dir=config['paths']['cache_dir'],\n",
    "    use_cache=config['cache']['enabled']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8900e932",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T19:49:36.045851Z",
     "iopub.status.busy": "2025-06-01T19:49:36.045548Z",
     "iopub.status.idle": "2025-06-01T19:49:36.092294Z",
     "shell.execute_reply": "2025-06-01T19:49:36.091492Z",
     "shell.execute_reply.started": "2025-06-01T19:49:36.045829Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Update configuration with memory-efficient settings\n",
    "config['training'].update({\n",
    "    'batch_size': 8,  # Reduced batch size\n",
    "    'gradient_accumulation_steps': 4,  # Add gradient accumulation\n",
    "    'prefetch_factor': 2  # Reduce prefetch factor\n",
    "})\n",
    "\n",
    "# Create data loaders with memory-efficient settings\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=config['training']['prefetch_factor'],\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "# Update validation loaders with the same settings\n",
    "val_easy_loader = DataLoader(\n",
    "    val_easy_dataset,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=config['training']['prefetch_factor'],\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_hard_loader = DataLoader(\n",
    "    val_hard_dataset,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=config['training']['prefetch_factor'],\n",
    "    persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcf8611",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T19:49:38.830843Z",
     "iopub.status.busy": "2025-06-01T19:49:38.830572Z",
     "iopub.status.idle": "2025-06-01T19:49:38.848595Z",
     "shell.execute_reply": "2025-06-01T19:49:38.847812Z",
     "shell.execute_reply.started": "2025-06-01T19:49:38.830821Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up model training...\n",
      "Using device: cuda\n",
      "Model training setup complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting up model training...\")\n",
    "# Initialize device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model\n",
    "model = SpeakerVerificationModel(\n",
    "    input_dim=config['audio']['n_mels'],\n",
    "    channels=config['model']['channels'],\n",
    "    embedding_dim=config['model']['embedding_dim'],\n",
    "    num_blocks=config['model']['num_blocks'],\n",
    "    num_speakers=len(train_dataset.speaker_to_idx)\n",
    ")\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Watch model in wandb\n",
    "wandb.watch(model)\n",
    "\n",
    "# Initialize optimizer and scheduler\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=config['training']['learning_rate'],\n",
    "    weight_decay=config['training']['weight_decay']\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=config['training']['num_epochs']\n",
    ")\n",
    "\n",
    "# Loss criterion for training\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "print(\"Model training setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f34d04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T19:49:41.912417Z",
     "iopub.status.busy": "2025-06-01T19:49:41.912160Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/150 (Train):   0%|          | 0/1890 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = SpeakerVerificationModel(\n",
    "    input_dim=config['audio']['n_mels'],\n",
    "    channels=config['model']['channels'],\n",
    "    embedding_dim=config['model']['embedding_dim'],\n",
    "    num_blocks=config['model']['num_blocks'],\n",
    "    num_speakers=len(train_dataset.speaker_to_idx)\n",
    ")\n",
    "\n",
    "# Watch model in wandb\n",
    "wandb.watch(model)\n",
    "\n",
    "# Validation function to compute EER\n",
    "def validate_eer(model, val_loader, device):\n",
    "    model.eval()\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            features1 = batch['features1'].to(device, non_blocking=True)\n",
    "            features2 = batch['features2'].to(device, non_blocking=True)\n",
    "            labels = batch['label'].to(device, non_blocking=True)\n",
    "            \n",
    "            # Extract embeddings for both utterances\n",
    "            embeddings1 = model.extract_embedding(features1)\n",
    "            embeddings2 = model.extract_embedding(features2)\n",
    "            \n",
    "            # Compute cosine similarity\n",
    "            similarities = F.cosine_similarity(embeddings1, embeddings2)\n",
    "            \n",
    "            # Move to CPU and convert to numpy for accumulation\n",
    "            all_scores.append(similarities.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            \n",
    "            # Clean up GPU memory\n",
    "            del features1, features2, embeddings1, embeddings2, similarities, labels\n",
    "    \n",
    "    # Concatenate all scores and labels\n",
    "    all_scores = np.concatenate(all_scores)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    \n",
    "    # Compute EER\n",
    "    eer, threshold = compute_eer(all_scores, all_labels)\n",
    "    return eer, threshold\n",
    "\n",
    "# Training loop\n",
    "num_epochs = config['training']['num_epochs']\n",
    "best_val_eer = float('inf')\n",
    "best_epoch = 0\n",
    "best_state_dict = None\n",
    "accumulation_steps = config['training']['gradient_accumulation_steps']\n",
    "\n",
    "# Clear GPU memory before starting\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} (Train)')):\n",
    "        try:\n",
    "            # Move data to GPU\n",
    "            features = batch['features'].to(device, non_blocking=True)\n",
    "            speaker_ids = batch['speaker_ids'].to(device, non_blocking=True)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(features, speaker_ids)\n",
    "            loss = criterion(logits, speaker_ids)\n",
    "            \n",
    "            # Scale loss for gradient accumulation\n",
    "            loss = loss / accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update weights if we've accumulated enough gradients\n",
    "            if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=3.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            train_loss += loss.item() * accumulation_steps\n",
    "            \n",
    "            # Log batch-level metrics less frequently\n",
    "            if batch_idx % 100 == 0:\n",
    "                wandb.log({\n",
    "                    'batch': epoch * len(train_loader) + batch_idx,\n",
    "                    'batch_loss': loss.item() * accumulation_steps\n",
    "                })\n",
    "            \n",
    "            # Clean up GPU memory\n",
    "            del features, speaker_ids, logits, loss\n",
    "            if batch_idx % 10 == 0:  # Periodically clear cache\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Make sure to update with any remaining gradients\n",
    "    if len(train_loader) % accumulation_steps != 0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=3.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    # Clear memory before validation\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Validation phase\n",
    "    val_easy_eer, easy_threshold = validate_eer(model, val_easy_loader, device)\n",
    "    val_hard_eer, hard_threshold = validate_eer(model, val_hard_loader, device)\n",
    "    \n",
    "    # Average EER from both validation sets\n",
    "    avg_val_eer = (val_easy_eer + val_hard_eer) / 2\n",
    "    \n",
    "    # Log epoch-level metrics\n",
    "    wandb.log({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': train_loss,\n",
    "        'val_easy_eer': val_easy_eer * 100,\n",
    "        'val_hard_eer': val_hard_eer * 100,\n",
    "        'avg_val_eer': avg_val_eer * 100,\n",
    "        'easy_threshold': easy_threshold,\n",
    "        'hard_threshold': hard_threshold,\n",
    "        'learning_rate': scheduler.get_last_lr()[0]\n",
    "    })\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "    print(f'Training Loss: {train_loss:.4f}')\n",
    "    print(f'Validation Easy EER: {val_easy_eer*100:.2f}% (threshold: {easy_threshold:.3f})')\n",
    "    print(f'Validation Hard EER: {val_hard_eer*100:.2f}% (threshold: {hard_threshold:.3f})')\n",
    "    print(f'Average Validation EER: {avg_val_eer*100:.2f}%')\n",
    "    \n",
    "    # Clear GPU memory before checkpoint saving\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Save best model based on average EER\n",
    "    if avg_val_eer < best_val_eer:\n",
    "        best_val_eer = avg_val_eer\n",
    "        best_epoch = epoch + 1\n",
    "        best_state_dict = model.state_dict().copy()\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_val_eer': best_val_eer,\n",
    "            'easy_threshold': easy_threshold,\n",
    "            'hard_threshold': hard_threshold\n",
    "        }\n",
    "        torch.save(checkpoint, f'{config[\"paths\"][\"checkpoint_dir\"]}/best_model.pth')\n",
    "        # Save model to wandb\n",
    "        wandb.save(f'{config[\"paths\"][\"checkpoint_dir\"]}/best_model.pth')\n",
    "        print(f'New best model saved! (Average EER: {best_val_eer*100:.2f}%)')\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step()\n",
    "\n",
    "print(f'\\nTraining completed!')\n",
    "print(f'Best validation EER: {best_val_eer*100:.2f}% at epoch {best_epoch}')\n",
    "\n",
    "# Log final best metrics\n",
    "wandb.run.summary.update({\n",
    "    'best_val_eer': best_val_eer * 100,  # Convert to percentage\n",
    "    'best_epoch': best_epoch\n",
    "})\n",
    "\n",
    "# Load best model for final use\n",
    "checkpoint = torch.load(f'{config[\"paths\"][\"checkpoint_dir\"]}/best_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Close wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39198626",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-01T19:47:38.073639Z",
     "iopub.status.idle": "2025-06-01T19:47:38.074090Z",
     "shell.execute_reply": "2025-06-01T19:47:38.073863Z",
     "shell.execute_reply.started": "2025-06-01T19:47:38.073846Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "final_model_path = '/kaggle/working/final_model.pth'\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"Training completed and model saved to {final_model_path}!\")\n",
    "print(f\"Best validation EER: {best_val_eer*100:.2f}% at epoch {best_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fba502",
   "metadata": {},
   "source": [
    "## Dataset Augmentation Setup\n",
    "\n",
    "### MUSAN Dataset Configuration (Optional)\n",
    "\n",
    "This section configures audio augmentation using the MUSAN dataset. The training will work without MUSAN, but having it enables better model robustness through:\n",
    "- Background noise augmentation\n",
    "- Music interference augmentation\n",
    "- Speech babble noise augmentation\n",
    "\n",
    "If you don't have the MUSAN dataset:\n",
    "1. Visit http://www.openslr.org/17/\n",
    "2. Download and extract to `/kaggle/input/musan-dataset/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a9882b",
   "metadata": {},
   "source": [
    "## Optional: Augmentation Quality Validation\n",
    "\n",
    "This section helps validate the quality of audio augmentation by visualizing and analyzing:\n",
    "1. Waveforms before/after augmentation\n",
    "2. Spectrograms to verify frequency characteristics\n",
    "3. Signal-to-Noise Ratio (SNR) statistics\n",
    "4. Augmentation type distribution\n",
    "\n",
    "You can skip this section if not using MUSAN augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca490cc1",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-01T19:47:38.076414Z",
     "iopub.status.idle": "2025-06-01T19:47:38.076948Z",
     "shell.execute_reply": "2025-06-01T19:47:38.076672Z",
     "shell.execute_reply.started": "2025-06-01T19:47:38.076654Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import librosa\n",
    "# import librosa.display\n",
    "# import numpy as np\n",
    "# from utils.augment import AudioAugmenter\n",
    "\n",
    "# # Create audio augmenter with our configuration\n",
    "# audio_augmenter = AudioAugmenter(\n",
    "#     sample_rate=config['audio']['sample_rate'],\n",
    "#     enabled=True,\n",
    "#     speed_perturb=augment_config.get('speed_perturb', True),\n",
    "#     noise_prob=augment_config.get('noise_prob', 0.5),\n",
    "#     noise_snr_range=augment_config['noise_snr_range'],  # Fixed parameter name\n",
    "#     reverb_prob=augment_config.get('reverb_prob', 0.5),\n",
    "#     musan_path=augment_config.get('musan_path'),\n",
    "#     noise_types=augment_config['noise_types']\n",
    "# )\n",
    "\n",
    "# def plot_audio_comparison(original, augmented, sr, title=\"Audio Comparison\"):\n",
    "#     fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "#     fig.suptitle(title)\n",
    "    \n",
    "#     # Original waveform\n",
    "#     axes[0,0].plot(original)\n",
    "#     axes[0,0].set_title('Original Waveform')\n",
    "    \n",
    "#     # Augmented waveform\n",
    "#     axes[0,1].plot(augmented)\n",
    "#     axes[0,1].set_title('Augmented Waveform')\n",
    "    \n",
    "#     # Original spectrogram\n",
    "#     D = librosa.amplitude_to_db(np.abs(librosa.stft(original)), ref=np.max)\n",
    "#     librosa.display.specshow(D, y_axis='log', x_axis='time', ax=axes[1,0])\n",
    "#     axes[1,0].set_title('Original Spectrogram')\n",
    "    \n",
    "#     # Augmented spectrogram\n",
    "#     D = librosa.amplitude_to_db(np.abs(librosa.stft(augmented)), ref=np.max)\n",
    "#     librosa.display.specshow(D, y_axis='log', x_axis='time', ax=axes[1,1])\n",
    "#     axes[1,1].set_title('Augmented Spectrogram')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     return fig\n",
    "\n",
    "# def compute_snr(original, noisy):\n",
    "#     noise = noisy - original\n",
    "#     signal_power = np.mean(original ** 2)\n",
    "#     noise_power = np.mean(noise ** 2)\n",
    "#     snr = 10 * np.log10(signal_power / noise_power)\n",
    "#     return snr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5513484e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-01T19:47:38.078938Z",
     "iopub.status.idle": "2025-06-01T19:47:38.079371Z",
     "shell.execute_reply": "2025-06-01T19:47:38.079168Z",
     "shell.execute_reply.started": "2025-06-01T19:47:38.079150Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Get a sample from the training dataset\n",
    "# sample_idx = 0\n",
    "# sample = train_dataset[sample_idx]\n",
    "# original_audio = sample['audio']\n",
    "\n",
    "# # Test different noise types\n",
    "# noise_types = ['background', 'music', 'babble']\n",
    "# figs = []\n",
    "\n",
    "# for noise_type in noise_types:\n",
    "#     # Apply specific noise augmentation\n",
    "#     augmented_audio = audio_augmenter.apply_noise(\n",
    "#         original_audio.numpy(), \n",
    "#         noise_type=noise_type,\n",
    "#         snr=np.random.uniform(*augment_config['snr_range'])\n",
    "#     )\n",
    "    \n",
    "#     # Calculate actual SNR\n",
    "#     actual_snr = compute_snr(original_audio.numpy(), augmented_audio)\n",
    "    \n",
    "#     # Plot comparison\n",
    "#     fig = plot_audio_comparison(\n",
    "#         original_audio.numpy(),\n",
    "#         augmented_audio,\n",
    "#         config['audio']['sample_rate'],\n",
    "#         f'Audio Comparison - {noise_type.capitalize()} Noise (SNR: {actual_snr:.2f} dB)'\n",
    "#     )\n",
    "#     figs.append(fig)\n",
    "    \n",
    "#     # Log augmented audio sample to wandb\n",
    "#     wandb.log({\n",
    "#         f'audio_samples/{noise_type}': wandb.Audio(\n",
    "#             augmented_audio,\n",
    "#             sample_rate=config['audio']['sample_rate'],\n",
    "#             caption=f'{noise_type.capitalize()} noise augmentation'\n",
    "#         )\n",
    "#     })\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0faf9b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-01T19:47:38.083601Z",
     "iopub.status.idle": "2025-06-01T19:47:38.084109Z",
     "shell.execute_reply": "2025-06-01T19:47:38.083845Z",
     "shell.execute_reply.started": "2025-06-01T19:47:38.083826Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Validate augmentation statistics\n",
    "# def validate_augmentation_stats(dataset, num_samples=100):\n",
    "#     snr_values = []\n",
    "#     noise_type_counts = {noise_type: 0 for noise_type in noise_types}\n",
    "#     total_augmented = 0\n",
    "\n",
    "#     for i in range(num_samples):\n",
    "#         sample = dataset[i]\n",
    "#         original_audio = sample['audio']\n",
    "        \n",
    "#         # Apply random augmentation as per training\n",
    "#         if np.random.random() < augment_config['noise_prob']:\n",
    "#             noise_type = np.random.choice(noise_types, p=[info['prob'] for info in augment_config['noise_types'].values()])\n",
    "#             snr = np.random.uniform(*augment_config['snr_range'])\n",
    "            \n",
    "#             augmented_audio = audio_augmenter.apply_noise(original_audio.numpy(), noise_type=noise_type, snr=snr)\n",
    "#             actual_snr = compute_snr(original_audio.numpy(), augmented_audio)\n",
    "            \n",
    "#             snr_values.append(actual_snr)\n",
    "#             noise_type_counts[noise_type] += 1\n",
    "#             total_augmented += 1\n",
    "    \n",
    "#     # Print statistics\n",
    "#     print(f\"Augmentation Statistics (over {num_samples} samples):\")\n",
    "#     print(f\"Total augmented: {total_augmented}/{num_samples} ({total_augmented/num_samples*100:.1f}%)\")\n",
    "#     print(\"\\nNoise type distribution:\")\n",
    "#     for noise_type, count in noise_type_counts.items():\n",
    "#         print(f\"{noise_type}: {count}/{total_augmented} ({count/total_augmented*100:.1f}% of augmented)\")\n",
    "    \n",
    "#     if snr_values:\n",
    "#         print(\"\\nSNR statistics:\")\n",
    "#         print(f\"Mean SNR: {np.mean(snr_values):.2f} dB\")\n",
    "#         print(f\"Std SNR: {np.std(snr_values):.2f} dB\")\n",
    "#         print(f\"Min SNR: {np.min(snr_values):.2f} dB\")\n",
    "#         print(f\"Max SNR: {np.max(snr_values):.2f} dB\")\n",
    "        \n",
    "#         # Log statistics to wandb\n",
    "#         wandb.log({\n",
    "#             'augmentation/mean_snr': np.mean(snr_values),\n",
    "#             'augmentation/std_snr': np.std(snr_values),\n",
    "#             'augmentation/augmentation_rate': total_augmented/num_samples\n",
    "#         })\n",
    "\n",
    "# # Run validation\n",
    "# validate_augmentation_stats(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747d220a",
   "metadata": {},
   "source": [
    "### MUSAN Dataset Structure\n",
    "\n",
    "The MUSAN dataset should be organized in the following structure at `/kaggle/input/musan-dataset/musan/`:\n",
    "\n",
    "```\n",
    "musan/\n",
    "├── noise/\n",
    "│   ├── free-sound/\n",
    "│   └── sound-bible/\n",
    "├── music/\n",
    "│   ├── fma/\n",
    "│   ├── fma-western-art/\n",
    "│   ├── hd-classical/\n",
    "│   ├── jamendo/\n",
    "│   ├── rfm/\n",
    "└── speech/\n",
    "    ├── librivox/\n",
    "    └── us-gov/\n",
    "```\n",
    "\n",
    "You can download the MUSAN dataset from:\n",
    "1. Visit http://www.openslr.org/17/\n",
    "2. Download the tar file (about 14GB compressed)\n",
    "3. Extract it to `/kaggle/input/musan-dataset/`\n",
    "\n",
    "The dataset contains:\n",
    "- **noise**: Various background noises and sound effects\n",
    "- **music**: Different genres and styles of music recordings\n",
    "- **speech**: Speech recordings from various sources\n",
    "\n",
    "Each category is used differently in our augmentation pipeline:\n",
    "- Background noise: Simulates real-world environments\n",
    "- Music: Creates challenging interference conditions\n",
    "- Speech: Generates realistic babble noise with multiple overlapped speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668ff32b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-01T19:47:38.084877Z",
     "iopub.status.idle": "2025-06-01T19:47:38.085384Z",
     "shell.execute_reply": "2025-06-01T19:47:38.085156Z",
     "shell.execute_reply.started": "2025-06-01T19:47:38.085133Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # First, let's verify the MUSAN dataset structure\n",
    "# def verify_musan_structure(root_path):\n",
    "#     expected_structure = {\n",
    "#         'noise': ['free-sound', 'sound-bible'],\n",
    "#         'music': ['fma', 'fma-western-art', 'hd-classical', 'jamendo', 'rfm'],\n",
    "#         'speech': ['librivox', 'us-gov']\n",
    "#     }\n",
    "    \n",
    "#     all_valid = True\n",
    "#     for category, subdirs in expected_structure.items():\n",
    "#         category_path = os.path.join(root_path, category)\n",
    "#         if not os.path.exists(category_path):\n",
    "#             logger.error(f\"Missing category directory: {category}\")\n",
    "#             all_valid = False\n",
    "#             continue\n",
    "            \n",
    "#         for subdir in subdirs:\n",
    "#             subdir_path = os.path.join(category_path, subdir)\n",
    "#             if not os.path.exists(subdir_path):\n",
    "#                 logger.warning(f\"Missing subdirectory: {category}/{subdir}\")\n",
    "#                 all_valid = False\n",
    "#             else:\n",
    "#                 # Count files to verify content\n",
    "#                 files = [f for f in os.listdir(subdir_path) if f.endswith('.wav')]\n",
    "#                 logger.info(f\"Found {len(files)} .wav files in {category}/{subdir}\")\n",
    "    \n",
    "#     return all_valid\n",
    "\n",
    "# # Verify MUSAN dataset structure\n",
    "# musan_valid = verify_musan_structure(musan_config['data_root'])\n",
    "# if not musan_valid:\n",
    "#     logger.warning(\"MUSAN dataset structure is incomplete. Some augmentations may not work as expected.\")\n",
    "# else:\n",
    "#     logger.info(\"MUSAN dataset structure verified successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6096635,
     "sourceId": 9920101,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7251391,
     "sourceId": 11565412,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7336252,
     "sourceId": 11688515,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7393408,
     "sourceId": 11776227,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3192482,
     "sourceId": 5539168,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
