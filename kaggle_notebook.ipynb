{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba995531",
   "metadata": {},
   "source": [
    "# Speaker Verification with ECAPA-TDNN on Kaggle GPU\n",
    "\n",
    "This notebook implements speaker verification using the ECAPA-TDNN architecture on Kaggle's GPU.\n",
    "\n",
    "## Table of Contents\n",
    "1. Setup and Installation\n",
    "2. Repository and Environment Configuration\n",
    "3. Dataset and Model Configuration\n",
    "4. Model Training\n",
    "5. Optional: MUSAN Augmentation Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a7752a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:33:35.305144Z",
     "iopub.status.busy": "2025-06-01T15:33:35.304857Z",
     "iopub.status.idle": "2025-06-01T15:33:38.584330Z",
     "shell.execute_reply": "2025-06-01T15:33:38.583080Z",
     "shell.execute_reply.started": "2025-06-01T15:33:35.305123Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Ignored the following yanked versions: 2.0.0\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torchaudio==0.9.0 (from versions: 2.0.1, 2.0.2, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torchaudio==0.9.0\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchaudio PyYAML soundfile librosa wandb\n",
    "!pip install matplotlib numpy tqdm\n",
    "\n",
    "print(\"Package installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f54167d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:39:26.548402Z",
     "iopub.status.busy": "2025-06-01T15:39:26.548074Z",
     "iopub.status.idle": "2025-06-01T15:39:27.399915Z",
     "shell.execute_reply": "2025-06-01T15:39:27.398791Z",
     "shell.execute_reply.started": "2025-06-01T15:39:26.548368Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n",
      "Cloning into 'speaker-verification'...\n",
      "remote: Enumerating objects: 33, done.\u001b[K\n",
      "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
      "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
      "remote: Total 33 (delta 3), reused 32 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (33/33), 20.13 KiB | 10.06 MiB/s, done.\n",
      "Resolving deltas: 100% (3/3), done.\n",
      "\n",
      "Directory structure:\n",
      ".:\n",
      "cli.py\t ecapa_tdnn\t   kaggle_notebook.ipynb  requirements.txt  train.py\n",
      "config\t evaluate.py\t   models\t\t  tester\t    utils\n",
      "dataset  global_config.py  README.md\t\t  trainer\t    verify.py\n",
      "\n",
      "./config:\n",
      "defaults.py  __init__.py\n",
      "\n",
      "./models:\n",
      "ecapa_tdnn.py  feature_extractor.py  __init__.py  modules.py\n",
      "\n",
      "./utils:\n",
      "augment.py  config.py  data.py\t__init__.py  metrics.py  training.py\n",
      "\n",
      "Project directory: /kaggle/working/speaker-verification\n",
      "Current working directory: /kaggle/working/speaker-verification\n",
      "\n",
      "Python path:\n",
      "  /kaggle/working/speaker-verification\n",
      "  /kaggle/working\n",
      "  /kaggle/lib/kagglegym\n",
      "  /kaggle/lib\n",
      "  /usr/lib/python311.zip\n",
      "  /usr/lib/python3.11\n",
      "  /usr/lib/python3.11/lib-dynload\n",
      "  \n",
      "  /usr/local/lib/python3.11/dist-packages\n",
      "  /usr/lib/python3/dist-packages\n",
      "  /usr/local/lib/python3.11/dist-packages/IPython/extensions\n",
      "  /usr/local/lib/python3.11/dist-packages/setuptools/_vendor\n",
      "  /root/.ipython\n",
      "  /tmp/tmpc1j13n0r\n"
     ]
    }
   ],
   "source": [
    "# Setup working directory and clone repository\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Start fresh in working directory\n",
    "%cd /kaggle/working\n",
    "!rm -rf speaker-verification\n",
    "\n",
    "# Clone repository\n",
    "!git clone https://github.com/mapotofu40/speaker-verification.git\n",
    "\n",
    "# Move config.py to avoid name conflict\n",
    "!mv speaker-verification/config.py speaker-verification/global_config.py\n",
    "\n",
    "# Set up Python path properly\n",
    "project_dir = Path('/kaggle/working/speaker-verification').absolute()\n",
    "\n",
    "# Clean up sys.path\n",
    "sys.path = [p for p in sys.path if 'speaker-verification/speaker-verification' not in p]\n",
    "\n",
    "# Add project directory to Python path if not already present\n",
    "if str(project_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(project_dir))\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir(project_dir)\n",
    "\n",
    "print(\"\\nDirectory structure:\")\n",
    "!ls -R\n",
    "\n",
    "print(f\"\\nProject directory: {project_dir}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"\\nPython path:\")\n",
    "for p in sys.path:\n",
    "    print(f\"  {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce5bba5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:39:48.896728Z",
     "iopub.status.busy": "2025-06-01T15:39:48.896377Z",
     "iopub.status.idle": "2025-06-01T15:39:48.936393Z",
     "shell.execute_reply": "2025-06-01T15:39:48.935116Z",
     "shell.execute_reply.started": "2025-06-01T15:39:48.896697Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported successfully!\n",
      "\n",
      "Module locations:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "<class 'dict'> is a built-in class",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35/137293188.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nModule locations:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"config.defaults: {inspect.getfile(BASE_CONFIG.__class__)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"models.ecapa_tdnn: {inspect.getfile(SpeakerVerificationModel)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"utils.data: {inspect.getfile(VietnamCelebDataset)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/package/package_importer.py\u001b[0m in \u001b[0;36m_patched_getfile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    729\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_imported_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_package_imported_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_orig_getfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/inspect.py\u001b[0m in \u001b[0;36mgetfile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'source code not available'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{!r} is a built-in class'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mobject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__func__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: <class 'dict'> is a built-in class"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import torch\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Clear any remaining path duplicates\n",
    "sys.path = list(dict.fromkeys(sys.path))\n",
    "\n",
    "# Verify imports work\n",
    "try:\n",
    "    from config.defaults import BASE_CONFIG\n",
    "    from models.ecapa_tdnn import SpeakerVerificationModel\n",
    "    from models.feature_extractor import FeatureExtractor\n",
    "    from utils.data import VietnamCelebDataset, collate_fn\n",
    "    from utils.training import train_model\n",
    "    from utils.metrics import compute_eer, cosine_similarity\n",
    "    from tqdm import tqdm\n",
    "    print(\"All modules imported successfully!\")\n",
    "    \n",
    "    # Print location of imported modules to verify correct paths\n",
    "    import inspect\n",
    "    print(\"\\nModule locations:\")\n",
    "    print(f\"config.defaults: {inspect.getfile(BASE_CONFIG.__class__)}\")\n",
    "    print(f\"models.ecapa_tdnn: {inspect.getfile(SpeakerVerificationModel)}\")\n",
    "    print(f\"utils.data: {inspect.getfile(VietnamCelebDataset)}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"Import error: {e}\")\n",
    "    print(\"\\nCurrent directory structure:\")\n",
    "    !pwd\n",
    "    !ls -R\n",
    "    print(f\"\\nPython path:\")\n",
    "    for p in sys.path:\n",
    "        print(f\"  {p}\")\n",
    "    raise\n",
    "\n",
    "# Verify GPU availability\n",
    "print(f\"\\nGPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba27bd8",
   "metadata": {},
   "source": [
    "## Weights & Biases Setup\n",
    "\n",
    "Before running the training, you need to:\n",
    "1. Create a free account at https://wandb.ai if you haven't already\n",
    "2. Get your API key from https://wandb.ai/authorize\n",
    "3. When you run the cell below, you'll see a prompt that says \"wandb: Please enter your credentials to login to wandb\"\n",
    "4. Paste your API key and press Enter\n",
    "\n",
    "Your API key will be securely stored for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f25bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Weights & Biases\n",
    "import wandb\n",
    "\n",
    "# This will prompt you to enter your API key\n",
    "wandb.login()\n",
    "\n",
    "# Initialize wandb run\n",
    "wandb.init(\n",
    "    project=\"speaker-verification\",\n",
    "    config={\n",
    "        \"architecture\": \"ECAPA-TDNN\",\n",
    "        \"dataset\": \"VietnamCeleb\",\n",
    "        **config['training'],  # Add training config\n",
    "        **config['audio'],     # Add audio processing config\n",
    "        **config['model']      # Add model architecture config\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bec11454",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:39:52.705604Z",
     "iopub.status.busy": "2025-06-01T15:39:52.704891Z",
     "iopub.status.idle": "2025-06-01T15:39:52.711365Z",
     "shell.execute_reply": "2025-06-01T15:39:52.710186Z",
     "shell.execute_reply.started": "2025-06-01T15:39:52.705573Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Configure paths for Kaggle\n",
    "config = BASE_CONFIG.copy()\n",
    "config['paths'].update({\n",
    "    'checkpoint_dir': '/kaggle/working/checkpoints',\n",
    "    'log_dir': '/kaggle/working/logs',\n",
    "    'cache_dir': '/kaggle/working/cache'\n",
    "})\n",
    "\n",
    "# Create necessary directories\n",
    "for path in config['paths'].values():\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86ecb45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:39:54.687365Z",
     "iopub.status.busy": "2025-06-01T15:39:54.687058Z",
     "iopub.status.idle": "2025-06-01T15:39:54.753676Z",
     "shell.execute_reply": "2025-06-01T15:39:54.752623Z",
     "shell.execute_reply.started": "2025-06-01T15:39:54.687343Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "data_config = {\n",
    "    'data_root': '/kaggle/input/vietnam-celeb-dataset/full-dataset',  # Update with your dataset path\n",
    "    'metadata_file': '/kaggle/input/vietnam-celeb-dataset/full-dataset/speaker-metadata.tsv',\n",
    "    'train_utterance_file': '/kaggle/input/asv-output/cleaned_utterances.txt',\n",
    "    'val_easy_utterance_file': '/kaggle/input/vietnam-celeb-dataset/vietnam-celeb-e.txt',\n",
    "    'val_hard_utterance_file': '/kaggle/input/vietnam-celeb-dataset/vietnam-celeb-h.txt'\n",
    "}\n",
    "\n",
    "# Create feature extractor\n",
    "feature_extractor = FeatureExtractor(\n",
    "    sample_rate=config['audio']['sample_rate'],\n",
    "    n_mels=config['audio']['n_mels']\n",
    ")\n",
    "\n",
    "# Augmentation configuration from config\n",
    "augment_config = config['augment'] if 'augment' in config else {\n",
    "    'enabled': True,\n",
    "    'speed_perturb': True,\n",
    "    'noise_prob': 0.5,\n",
    "    'noise_snr_range': [5, 20],\n",
    "    'reverb_prob': 0.5\n",
    "}\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = VietnamCelebDataset(\n",
    "    data_root=data_config['data_root'],\n",
    "    metadata_file=data_config['metadata_file'],\n",
    "    utterance_file=data_config['train_utterance_file'],\n",
    "    feature_extractor=feature_extractor,\n",
    "    cache_dir=config['paths']['cache_dir'],\n",
    "    use_cache=config['cache']['enabled'],\n",
    "    augment=augment_config['enabled'],\n",
    "    augment_config=augment_config,  # Now includes MUSAN configuration\n",
    "    musan_path=augment_config['musan_path']  # Add MUSAN path\n",
    ")\n",
    "\n",
    "val_easy_dataset = VietnamCelebDataset(\n",
    "    data_root=data_config['data_root'],\n",
    "    metadata_file=data_config['metadata_file'],\n",
    "    utterance_file=data_config['val_utterance_file'],\n",
    "    feature_extractor=feature_extractor,\n",
    "    cache_dir=config['paths']['cache_dir'],\n",
    "    use_cache=config['cache']['enabled'],\n",
    "    augment=False  # No augmentation for validation\n",
    ")\n",
    "\n",
    "val_hard_dataset = VietnamCelebDataset(\n",
    "    data_root=data_config['data_root'],\n",
    "    metadata_file=data_config['metadata_file'],\n",
    "    utterance_file=data_config['test_utterance_file'],\n",
    "    feature_extractor=feature_extractor,\n",
    "    cache_dir=config['paths']['cache_dir'],\n",
    "    use_cache=config['cache']['enabled'],\n",
    "    augment=False  # No augmentation for validation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8900e932",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:39:57.536233Z",
     "iopub.status.busy": "2025-06-01T15:39:57.535278Z",
     "iopub.status.idle": "2025-06-01T15:39:57.549360Z",
     "shell.execute_reply": "2025-06-01T15:39:57.548406Z",
     "shell.execute_reply.started": "2025-06-01T15:39:57.536193Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,  # Reduce for Kaggle\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_easy_loader = DataLoader(\n",
    "    val_easy_dataset,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,  # Reduce for Kaggle\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_hard_loader = DataLoader(\n",
    "    val_hard_dataset,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,  # Reduce for Kaggle\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f34d04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:39:59.514514Z",
     "iopub.status.busy": "2025-06-01T15:39:59.514163Z",
     "iopub.status.idle": "2025-06-01T15:40:13.741187Z",
     "shell.execute_reply": "2025-06-01T15:40:13.739802Z",
     "shell.execute_reply.started": "2025-06-01T15:39:59.514488Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/150 (Train):   0%|          | 8/1701 [00:14<49:55,  1.77s/it, loss=13.7]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35/2316878707.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m model = train_model(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kaggle/working/speaker-verification/utils/training.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, learning_rate, weight_decay, device, checkpoint_dir, resume_from, checkpoint_interval)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = SpeakerVerificationModel(\n",
    "    input_dim=config['audio']['n_mels'],\n",
    "    channels=config['model']['channels'],\n",
    "    embedding_dim=config['model']['embedding_dim'],\n",
    "    num_blocks=config['model']['num_blocks'],\n",
    "    num_speakers=len(train_dataset.speaker_to_idx)\n",
    ")\n",
    "\n",
    "# Watch model in wandb\n",
    "wandb.watch(model)\n",
    "\n",
    "# Validation function to compute EER\n",
    "def validate_eer(model, val_loader, device):\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "    all_speaker_ids = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            features = batch['features'].to(device)\n",
    "            speaker_ids = batch['speaker_ids'].to(device)\n",
    "            \n",
    "            # Extract embeddings\n",
    "            embeddings = model.extract_embedding(features)\n",
    "            \n",
    "            all_embeddings.append(embeddings)\n",
    "            all_speaker_ids.append(speaker_ids)\n",
    "    \n",
    "    # Concatenate all embeddings and speaker IDs\n",
    "    all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "    all_speaker_ids = torch.cat(all_speaker_ids, dim=0)\n",
    "    \n",
    "    # Compute all pairwise cosine similarities\n",
    "    similarities = cosine_similarity(all_embeddings.unsqueeze(1), all_embeddings.unsqueeze(0))\n",
    "    \n",
    "    # Create labels (1 for same speaker, 0 for different speaker)\n",
    "    labels = (all_speaker_ids.unsqueeze(1) == all_speaker_ids.unsqueeze(0)).float()\n",
    "    \n",
    "    # Compute EER\n",
    "    eer, threshold = compute_eer(similarities.cpu().numpy(), labels.cpu().numpy())\n",
    "    return eer, threshold\n",
    "\n",
    "# Training loop\n",
    "num_epochs = config['training']['num_epochs']\n",
    "best_val_eer = float('inf')\n",
    "best_epoch = 0\n",
    "best_state_dict = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} (Train)')):\n",
    "        features = batch['features'].to(device)\n",
    "        speaker_ids = batch['speaker_ids'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(features, speaker_ids)\n",
    "        loss = criterion(logits, speaker_ids)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Log batch-level metrics\n",
    "        wandb.log({\n",
    "            'batch': epoch * len(train_loader) + batch_idx,\n",
    "            'batch_loss': loss.item()\n",
    "        })\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    # Validation phase\n",
    "    val_easy_eer, easy_threshold = validate_eer(model, val_easy_loader, device)\n",
    "    val_hard_eer, hard_threshold = validate_eer(model, val_hard_loader, device)\n",
    "    \n",
    "    # Average EER from both validation sets\n",
    "    avg_val_eer = (val_easy_eer + val_hard_eer) / 2\n",
    "    \n",
    "    # Log epoch-level metrics\n",
    "    wandb.log({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': train_loss,\n",
    "        'val_easy_eer': val_easy_eer * 100,  # Convert to percentage\n",
    "        'val_hard_eer': val_hard_eer * 100,  # Convert to percentage\n",
    "        'avg_val_eer': avg_val_eer * 100,    # Convert to percentage\n",
    "        'easy_threshold': easy_threshold,\n",
    "        'hard_threshold': hard_threshold,\n",
    "        'learning_rate': scheduler.get_last_lr()[0]\n",
    "    })\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "    print(f'Training Loss: {train_loss:.4f}')\n",
    "    print(f'Validation Easy EER: {val_easy_eer*100:.2f}% (threshold: {easy_threshold:.3f})')\n",
    "    print(f'Validation Hard EER: {val_hard_eer*100:.2f}% (threshold: {hard_threshold:.3f})')\n",
    "    print(f'Average Validation EER: {avg_val_eer*100:.2f}%')\n",
    "    \n",
    "    # Save best model based on average EER\n",
    "    if avg_val_eer < best_val_eer:\n",
    "        best_val_eer = avg_val_eer\n",
    "        best_epoch = epoch + 1\n",
    "        best_state_dict = model.state_dict().copy()\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_val_eer': best_val_eer,\n",
    "            'easy_threshold': easy_threshold,\n",
    "            'hard_threshold': hard_threshold\n",
    "        }\n",
    "        torch.save(checkpoint, f'{config[\"paths\"][\"checkpoint_dir\"]}/best_model.pth')\n",
    "        # Save model to wandb\n",
    "        wandb.save(f'{config[\"paths\"][\"checkpoint_dir\"]}/best_model.pth')\n",
    "        print(f'New best model saved! (Average EER: {best_val_eer*100:.2f}%)')\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step()\n",
    "\n",
    "print(f'\\nTraining completed!')\n",
    "print(f'Best validation EER: {best_val_eer*100:.2f}% at epoch {best_epoch}')\n",
    "\n",
    "# Log final best metrics\n",
    "wandb.run.summary.update({\n",
    "    'best_val_eer': best_val_eer * 100,  # Convert to percentage\n",
    "    'best_epoch': best_epoch\n",
    "})\n",
    "\n",
    "# Load best model for final use\n",
    "checkpoint = torch.load(f'{config[\"paths\"][\"checkpoint_dir\"]}/best_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Close wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcf8611",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Setting up model training...\")\n",
    "# Initialize optimizer, criterion and move model to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize optimizer and scheduler\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=config['training']['learning_rate'],\n",
    "    weight_decay=config['training']['weight_decay']\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=config['training']['num_epochs']\n",
    ")\n",
    "\n",
    "# Loss criterion for training\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Model training setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39198626",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-01T15:33:46.968361Z",
     "iopub.status.idle": "2025-06-01T15:33:46.969240Z",
     "shell.execute_reply": "2025-06-01T15:33:46.969106Z",
     "shell.execute_reply.started": "2025-06-01T15:33:46.969091Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "final_model_path = '/kaggle/working/final_model.pth'\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"Training completed and model saved to {final_model_path}!\")\n",
    "print(f\"Best validation EER: {best_val_eer*100:.2f}% at epoch {best_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fba502",
   "metadata": {},
   "source": [
    "## Dataset Augmentation Setup\n",
    "\n",
    "### MUSAN Dataset Configuration (Optional)\n",
    "\n",
    "This section configures audio augmentation using the MUSAN dataset. The training will work without MUSAN, but having it enables better model robustness through:\n",
    "- Background noise augmentation\n",
    "- Music interference augmentation\n",
    "- Speech babble noise augmentation\n",
    "\n",
    "If you don't have the MUSAN dataset:\n",
    "1. Visit http://www.openslr.org/17/\n",
    "2. Download and extract to `/kaggle/input/musan-dataset/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8808d731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MUSAN dataset configuration\n",
    "musan_config = {\n",
    "    'data_root': '/kaggle/input/musan-dataset/musan', # Update this path to your MUSAN dataset location\n",
    "    'noise_dir': 'noise',\n",
    "    'music_dir': 'music',\n",
    "    'speech_dir': 'speech',\n",
    "    'sample_rate': config['audio']['sample_rate'],\n",
    "    'min_snr_db': 5,\n",
    "    'max_snr_db': 20\n",
    "}\n",
    "\n",
    "# Update augmentation configuration with MUSAN parameters\n",
    "augment_config.update({\n",
    "    'musan_path': musan_config['data_root'],\n",
    "    'noise_prob': 0.6,  # Probability of applying noise augmentation\n",
    "    'noise_types': {\n",
    "        'background': {'prob': 0.3, 'path': os.path.join(musan_config['data_root'], musan_config['noise_dir'])},\n",
    "        'music': {'prob': 0.4, 'path': os.path.join(musan_config['data_root'], musan_config['music_dir'])},\n",
    "        'babble': {'prob': 0.3, 'path': os.path.join(musan_config['data_root'], musan_config['speech_dir'])}\n",
    "    },\n",
    "    'snr_range': [musan_config['min_snr_db'], musan_config['max_snr_db']]\n",
    "})\n",
    "\n",
    "# Validate MUSAN paths\n",
    "for noise_type, info in augment_config['noise_types'].items():\n",
    "    if not os.path.exists(info['path']):\n",
    "        logger.warning(f\"MUSAN {noise_type} path does not exist: {info['path']}\")\n",
    "    else:\n",
    "        logger.info(f\"MUSAN {noise_type} path validated: {info['path']}\")\n",
    "\n",
    "# Update wandb config with MUSAN parameters\n",
    "wandb.config.update({\n",
    "    'musan': {\n",
    "        'noise_prob': augment_config['noise_prob'],\n",
    "        'noise_types': list(augment_config['noise_types'].keys()),\n",
    "        'snr_range': augment_config['snr_range']\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a9882b",
   "metadata": {},
   "source": [
    "## Optional: Augmentation Quality Validation\n",
    "\n",
    "This section helps validate the quality of audio augmentation by visualizing and analyzing:\n",
    "1. Waveforms before/after augmentation\n",
    "2. Spectrograms to verify frequency characteristics\n",
    "3. Signal-to-Noise Ratio (SNR) statistics\n",
    "4. Augmentation type distribution\n",
    "\n",
    "You can skip this section if not using MUSAN augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca490cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "from utils.augment import AudioAugmenter\n",
    "\n",
    "# Create audio augmenter with our configuration\n",
    "audio_augmenter = AudioAugmenter(\n",
    "    sample_rate=config['audio']['sample_rate'],\n",
    "    **augment_config\n",
    ")\n",
    "\n",
    "def plot_audio_comparison(original, augmented, sr, title=\"Audio Comparison\"):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(title)\n",
    "    \n",
    "    # Original waveform\n",
    "    axes[0,0].plot(original)\n",
    "    axes[0,0].set_title('Original Waveform')\n",
    "    \n",
    "    # Augmented waveform\n",
    "    axes[0,1].plot(augmented)\n",
    "    axes[0,1].set_title('Augmented Waveform')\n",
    "    \n",
    "    # Original spectrogram\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(original)), ref=np.max)\n",
    "    librosa.display.specshow(D, y_axis='log', x_axis='time', ax=axes[1,0])\n",
    "    axes[1,0].set_title('Original Spectrogram')\n",
    "    \n",
    "    # Augmented spectrogram\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(augmented)), ref=np.max)\n",
    "    librosa.display.specshow(D, y_axis='log', x_axis='time', ax=axes[1,1])\n",
    "    axes[1,1].set_title('Augmented Spectrogram')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def compute_snr(original, noisy):\n",
    "    noise = noisy - original\n",
    "    signal_power = np.mean(original ** 2)\n",
    "    noise_power = np.mean(noise ** 2)\n",
    "    snr = 10 * np.log10(signal_power / noise_power)\n",
    "    return snr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5513484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample from the training dataset\n",
    "sample_idx = 0\n",
    "sample = train_dataset[sample_idx]\n",
    "original_audio = sample['audio']\n",
    "\n",
    "# Test different noise types\n",
    "noise_types = ['background', 'music', 'babble']\n",
    "figs = []\n",
    "\n",
    "for noise_type in noise_types:\n",
    "    # Apply specific noise augmentation\n",
    "    augmented_audio = audio_augmenter.apply_noise(\n",
    "        original_audio.numpy(), \n",
    "        noise_type=noise_type,\n",
    "        snr=np.random.uniform(*augment_config['snr_range'])\n",
    "    )\n",
    "    \n",
    "    # Calculate actual SNR\n",
    "    actual_snr = compute_snr(original_audio.numpy(), augmented_audio)\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig = plot_audio_comparison(\n",
    "        original_audio.numpy(),\n",
    "        augmented_audio,\n",
    "        config['audio']['sample_rate'],\n",
    "        f'Audio Comparison - {noise_type.capitalize()} Noise (SNR: {actual_snr:.2f} dB)'\n",
    "    )\n",
    "    figs.append(fig)\n",
    "    \n",
    "    # Log augmented audio sample to wandb\n",
    "    wandb.log({\n",
    "        f'audio_samples/{noise_type}': wandb.Audio(\n",
    "            augmented_audio,\n",
    "            sample_rate=config['audio']['sample_rate'],\n",
    "            caption=f'{noise_type.capitalize()} noise augmentation'\n",
    "        )\n",
    "    })\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0faf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate augmentation statistics\n",
    "def validate_augmentation_stats(dataset, num_samples=100):\n",
    "    snr_values = []\n",
    "    noise_type_counts = {noise_type: 0 for noise_type in noise_types}\n",
    "    total_augmented = 0\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        sample = dataset[i]\n",
    "        original_audio = sample['audio']\n",
    "        \n",
    "        # Apply random augmentation as per training\n",
    "        if np.random.random() < augment_config['noise_prob']:\n",
    "            noise_type = np.random.choice(noise_types, p=[info['prob'] for info in augment_config['noise_types'].values()])\n",
    "            snr = np.random.uniform(*augment_config['snr_range'])\n",
    "            \n",
    "            augmented_audio = audio_augmenter.apply_noise(original_audio.numpy(), noise_type=noise_type, snr=snr)\n",
    "            actual_snr = compute_snr(original_audio.numpy(), augmented_audio)\n",
    "            \n",
    "            snr_values.append(actual_snr)\n",
    "            noise_type_counts[noise_type] += 1\n",
    "            total_augmented += 1\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Augmentation Statistics (over {num_samples} samples):\")\n",
    "    print(f\"Total augmented: {total_augmented}/{num_samples} ({total_augmented/num_samples*100:.1f}%)\")\n",
    "    print(\"\\nNoise type distribution:\")\n",
    "    for noise_type, count in noise_type_counts.items():\n",
    "        print(f\"{noise_type}: {count}/{total_augmented} ({count/total_augmented*100:.1f}% of augmented)\")\n",
    "    \n",
    "    if snr_values:\n",
    "        print(\"\\nSNR statistics:\")\n",
    "        print(f\"Mean SNR: {np.mean(snr_values):.2f} dB\")\n",
    "        print(f\"Std SNR: {np.std(snr_values):.2f} dB\")\n",
    "        print(f\"Min SNR: {np.min(snr_values):.2f} dB\")\n",
    "        print(f\"Max SNR: {np.max(snr_values):.2f} dB\")\n",
    "        \n",
    "        # Log statistics to wandb\n",
    "        wandb.log({\n",
    "            'augmentation/mean_snr': np.mean(snr_values),\n",
    "            'augmentation/std_snr': np.std(snr_values),\n",
    "            'augmentation/augmentation_rate': total_augmented/num_samples\n",
    "        })\n",
    "\n",
    "# Run validation\n",
    "validate_augmentation_stats(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747d220a",
   "metadata": {},
   "source": [
    "### MUSAN Dataset Structure\n",
    "\n",
    "The MUSAN dataset should be organized in the following structure at `/kaggle/input/musan-dataset/musan/`:\n",
    "\n",
    "```\n",
    "musan/\n",
    "├── noise/\n",
    "│   ├── free-sound/\n",
    "│   └── sound-bible/\n",
    "├── music/\n",
    "│   ├── fma/\n",
    "│   ├── fma-western-art/\n",
    "│   ├── hd-classical/\n",
    "│   ├── jamendo/\n",
    "│   ├── rfm/\n",
    "└── speech/\n",
    "    ├── librivox/\n",
    "    └── us-gov/\n",
    "```\n",
    "\n",
    "You can download the MUSAN dataset from:\n",
    "1. Visit http://www.openslr.org/17/\n",
    "2. Download the tar file (about 14GB compressed)\n",
    "3. Extract it to `/kaggle/input/musan-dataset/`\n",
    "\n",
    "The dataset contains:\n",
    "- **noise**: Various background noises and sound effects\n",
    "- **music**: Different genres and styles of music recordings\n",
    "- **speech**: Speech recordings from various sources\n",
    "\n",
    "Each category is used differently in our augmentation pipeline:\n",
    "- Background noise: Simulates real-world environments\n",
    "- Music: Creates challenging interference conditions\n",
    "- Speech: Generates realistic babble noise with multiple overlapped speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668ff32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's verify the MUSAN dataset structure\n",
    "def verify_musan_structure(root_path):\n",
    "    expected_structure = {\n",
    "        'noise': ['free-sound', 'sound-bible'],\n",
    "        'music': ['fma', 'fma-western-art', 'hd-classical', 'jamendo', 'rfm'],\n",
    "        'speech': ['librivox', 'us-gov']\n",
    "    }\n",
    "    \n",
    "    all_valid = True\n",
    "    for category, subdirs in expected_structure.items():\n",
    "        category_path = os.path.join(root_path, category)\n",
    "        if not os.path.exists(category_path):\n",
    "            logger.error(f\"Missing category directory: {category}\")\n",
    "            all_valid = False\n",
    "            continue\n",
    "            \n",
    "        for subdir in subdirs:\n",
    "            subdir_path = os.path.join(category_path, subdir)\n",
    "            if not os.path.exists(subdir_path):\n",
    "                logger.warning(f\"Missing subdirectory: {category}/{subdir}\")\n",
    "                all_valid = False\n",
    "            else:\n",
    "                # Count files to verify content\n",
    "                files = [f for f in os.listdir(subdir_path) if f.endswith('.wav')]\n",
    "                logger.info(f\"Found {len(files)} .wav files in {category}/{subdir}\")\n",
    "    \n",
    "    return all_valid\n",
    "\n",
    "# Verify MUSAN dataset structure\n",
    "musan_valid = verify_musan_structure(musan_config['data_root'])\n",
    "if not musan_valid:\n",
    "    logger.warning(\"MUSAN dataset structure is incomplete. Some augmentations may not work as expected.\")\n",
    "else:\n",
    "    logger.info(\"MUSAN dataset structure verified successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6096635,
     "sourceId": 9920101,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7251391,
     "sourceId": 11565412,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7336252,
     "sourceId": 11688515,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7393408,
     "sourceId": 11776227,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
